{"cells":[{"cell_type":"markdown","metadata":{"id":"fEGfKHNl1B8W"},"source":["**Modelo de red neuronal LSTM**\n","\n","Este archivo consta de los codigos y conclusiones de:\n","* 1.Carga de librerias y datos\n","* 2.Normalizar Datos.\n","* 3.Preparar datos para realizar aprendizaje supervizado.\n","* 4.Modelo LSTM\n","* 5.Evaluacion del modelo\n"]},{"cell_type":"markdown","metadata":{"id":"6RP-4WruJKgE"},"source":["#1.Carga de Librerias y Datos\n"]},{"cell_type":"markdown","metadata":{"id":"nVnMhcSoSlSV"},"source":["*Se importan los módulos necesarios para trabajar*"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4231,"status":"ok","timestamp":1692019545229,"user":{"displayName":"Benjamin Cruz Valenzuela","userId":"03062494153162488569"},"user_tz":240},"id":"G7i-wKjBODjL"},"outputs":[],"source":["#Pandas es utilizado para leer los set de datos\n","import pandas as pd\n","#Numpy es utilizado para generar las series de datos a graficar\n","import numpy as np\n","#Seaborn es utilizado para generar los gráficos\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","#Se importan modulos estadisticos para generar test de hipotesis, entre otros\n","from sklearn.preprocessing import StandardScaler\n","#Módulos implementa funciones que evalúan el error de predicción para propósitos específicos\n","from sklearn.metrics import mean_absolute_error as mae\n","from sklearn.metrics import mean_absolute_percentage_error as mape\n","from sklearn.metrics import mean_squared_error as mse\n","#Ignorar warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#Dividir arreglos o matrices en subconjuntos aleatorios de tren y prueba\n","from sklearn.model_selection import train_test_split\n","\n","#Biblioteca de Redes Neuronales\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential,model_from_json\n","from keras.layers import Dropout, LSTM, Dense, Activation,Input\n","from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.wrappers.scikit_learn import KerasRegressor\n","\n","from hyperopt import Trials, STATUS_OK, tpe, hp, fmin, space_eval\n","from sklearn.model_selection import cross_val_score, KFold, cross_val_predict, TimeSeriesSplit\n","import time,random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2EV65dSNeQz"},"outputs":[],"source":["# Para acceder a los archivos del gdrive\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0G62iH51OBl0"},"outputs":[],"source":["cd /content/gdrive/MyDrive/Tesis/Datos"]},{"cell_type":"markdown","metadata":{"id":"M7M1WueLhoz-"},"source":["Se obtiene conjunto de datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vMxCeqUODn2"},"outputs":[],"source":["df=pd.read_csv('df.csv')\n","df=df.drop(['Year', 'Week', 'Day','Month','Size','Type'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTDCpWgvWbjB"},"outputs":[],"source":["df.set_index('Date', inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrzSigQgbnyy"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"42gYrb5UYPpE"},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ji3Tx9YAvfD"},"outputs":[],"source":["#Setear semilla\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"NnR5PQgjnb0w"},"source":["#2. Obtener series de Tiempo"]},{"cell_type":"markdown","metadata":{"id":"vGjXG_s5KPdb"},"source":["*Se obtiene lista de dataframe ordenados por Store y Dept*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSYNxD4rEp5d"},"outputs":[],"source":["series_time=[]\n","lista_Store=df.Store.unique()\n","lista_Store.sort()\n","lista_dept=df.Dept.unique()\n","lista_dept.sort()\n","\n","for i in lista_Store:\n","  for j in lista_dept:\n","    #lista=[]\n","    test=df[(df.Store==i) & (df.Dept==j)]\n","    if(test.empty!=True):\n","        series_time.append(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFIWVeUIghg-"},"outputs":[],"source":["len(series_time)"]},{"cell_type":"markdown","metadata":{"id":"jm_8gyGw2l8v"},"source":["#3.Normalizar base de datos"]},{"cell_type":"markdown","metadata":{"id":"2DFMsd5_aLHt"},"source":["El **método de puntuación z** (a menudo llamado estandarización ) transforma los datos en una distribución con una media de 0 y una desviación estándar de 1 . Cada valor estandarizado se calcula restando la media de la característica correspondiente y luego dividiendo por la desviación estándar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luaoD1iQhL9e"},"outputs":[],"source":["#Seleccion de caracteristicas\n","features =features = [feature for feature in df.columns if feature not in ('Store','Dept')]\n","\n","#Se define escalado\n","std_scaler = StandardScaler()\n","\n","series_time_scaled=[]\n","\n","#Transformacion\n","for serie in series_time:\n","  for i in features:\n","    serie[i]=std_scaler.fit_transform(serie[i].values.reshape(-1,1))\n","  series_time_scaled.append(serie)\n","\n","for i in features:\n","  df[i] = std_scaler.fit_transform(df[i].values.reshape(-1,1))\n","\n","series_time_scaled[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYU3gA6Pivm9"},"outputs":[],"source":["len(series_time_scaled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dYMjkA-bt38"},"outputs":[],"source":["series_time_scaled=random.sample(series_time_scaled,20)"]},{"cell_type":"markdown","metadata":{"id":"NDnWUzQzmqBj"},"source":["#4.Preparar datos para realizar aprendizaje supervizado."]},{"cell_type":"markdown","metadata":{"id":"7_LgpS9oHs1a"},"source":["La idea es modelar cada valor en función de los valores recientes anteriores, dado un retardo de tiempo dado. **Los valores futuros de una variable en una serie de tiempo dependen de sus propios rezagos y de los rezagos de otras variables.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnqhyHufM9Rm"},"outputs":[],"source":["def time_delay_embedding(series: pd.Series, n_lags: int, horizon: int):\n","    \"\"\"\n","    Incrustación de retardo de tiempo\n","    :param series: serie de tiempo como objeto de pandas\n","    :param n_lags: número de valores pasados para usar como variables explicativas\n","    :param horizon: horizonte de pronostico\n","    :return:pd.DataFrame con series temporales reconstruidas\n","    \"\"\"\n","    assert isinstance(series, pd.Series)\n","\n","    if series.name is None:\n","        name = 'Series'\n","    else:\n","        name = series.name\n","\n","    n_lags_iter = list(range(n_lags, -horizon, -1))\n","\n","    serie_time_delay = [series.shift(i) for i in n_lags_iter]\n","    serie_time_delay = pd.concat(serie_time_delay, axis=1).dropna()\n","    serie_time_delay.columns = [f'{name}(t-{j - 1})'\n","                 if j > 0 else f'{name}(t+{np.abs(j) + 1})'\n","                 for j in n_lags_iter]\n","\n","    return serie_time_delay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDaQe8fbMLGd"},"outputs":[],"source":["series_predic=[]\n","series_target=[]\n","for serie in series_time_scaled:\n","  serie_split = []\n","  for columna in serie:\n","    col_df = time_delay_embedding(\n","        serie[columna],     #Serie de tiempo\n","        n_lags=1,           #Numero de retrasos\n","        horizon=1           # Horizonte de prediccion\n","          )\n","    serie_split.append(col_df)\n","\n","  serie_df = pd.concat(serie_split, axis=1).dropna()\n","  predictor_variables = serie_df.columns.str.contains('\\(t\\-')\n","  target_variables = serie_df.columns.str.contains('Weekly_Sales\\(t\\+')\n","\n","  predictor_variables = serie_df.iloc[:, predictor_variables]\n","  target_variables = serie_df.iloc[:, target_variables]\n","  series_predic.append(predictor_variables)\n","  series_target.append(target_variables)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECpeVlq9GD5e"},"outputs":[],"source":["#Ejemplo de variables de prediccion de una serie\n","series_predic[0].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g19QW1Zpg6Zi"},"outputs":[],"source":["#Ejemplo de variables objetivo de una serie\n","series_target[0].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ET3jqe2ASan"},"outputs":[],"source":["#Se separa conjunto en entrenamiento y prueba; sin aleatoriedad\n","#Dejando un %20 de la data para test\n","X_train=pd.DataFrame()\n","X_test=pd.DataFrame()\n","Y_train=pd.DataFrame()\n","Y_test=pd.DataFrame()\n","\n","for serie,target in zip(series_predic,series_target):\n","  X_train_i, X_test_i, Y_train_i, Y_test_i = train_test_split(serie, target, test_size=0.2, shuffle=False)\n","  X_train=pd.concat([X_train, X_train_i])\n","  X_test=pd.concat([X_test, X_test_i])\n","  Y_train=pd.concat([Y_train, Y_train_i])\n","  Y_test=pd.concat([Y_test, Y_test_i])\n","\n","shape_x_test=X_test.shape\n","shape_y_test=Y_test.shape\n","\n","print(\"Separacion de datos terminada!\")"]},{"cell_type":"markdown","metadata":{"id":"7cWkYwLPYSLg"},"source":["#5.LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkRDxgUebd8d"},"outputs":[],"source":["# Ajustar parámetros para reproducibilidad del entrenamiento\n","#tf.random.set_seed(123)\n","keras.utils.set_random_seed(123)\n","tf.config.experimental.enable_op_determinism()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oOsY98FC-1q4"},"outputs":[],"source":["#Se da formato de entradas como: Un tensor 3D con la forma [batch, timesteps, feature]\n","X_train=np.array(X_train)\n","X_train = X_train.reshape((X_train.shape[0],1,X_train.shape[1]))\n","\n","Y_train_s=np.array(Y_train)\n","Y_train_s = Y_train_s.reshape((Y_train_s.shape[0],1,Y_train_s.shape[1]))\n","\n","X_test=np.array(X_test)\n","X_test = X_test.reshape((X_test.shape[0],1,X_test.shape[1]))\n","\n","Y_test=np.array(Y_test)\n","Y_test = Y_test.reshape((Y_test.shape[0], 1, Y_test.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgHEgNTi75c4"},"outputs":[],"source":["# Función para crear el modelo\n","def create_model(neurons, dropout,layers,learning_rate):\n","\n","    #Creacion de la arquitectura LSTM\n","    model = Sequential()\n","    #Capas ocultas\n","    for i in range(layers):\n","        model.add(LSTM(units=neurons, return_sequences=(layers-1), input_shape=(X_train.shape[1],X_train.shape[2]) ))\n","        model.add(Dropout(dropout))\n","    #Capa de salida\n","    model.add(Dense(1))\n","\n","    #Se configura el proceso de entrenamiento del modelo y metricas de evaluacion\n","    model.compile(loss='mean_squared_error',\n","                  optimizer=Adam(learning_rate=learning_rate),\n","                  metrics = [tf.keras.metrics.MeanSquaredError(),\n","                            tf.keras.metrics.RootMeanSquaredError(),\n","                            tf.keras.metrics.MeanAbsoluteError(),\n","                            tf.keras.metrics.MeanAbsolutePercentageError()]\n","                  )\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcMm11JC6jlm"},"outputs":[],"source":["def objective(params):\n","\n","    # Definir los hiperparámetros a optimizar\n","    epochs = params['epochs']\n","    batch_size = params['batch_size']\n","    optimizer = params['optimizer']\n","    neurons  = params['neurons']\n","    dropout  = params['dropout']\n","    layers  = params['layers']\n","    learning_rate = params['learning_rate']\n","\n","\n","\n","    #kf = KFold(n_splits=5)\n","    tscv = TimeSeriesSplit(n_splits=5)\n","    scores = []\n","    times=[]\n","    models=[]\n","    scores_test=[]\n","\n","    #Se detiene el entrenamiento en el momento que se observe un incremento en el valor del error de validación.\n","    early_stop = EarlyStopping(monitor='val_loss', patience=30, mode='min',restore_best_weights=True)\n","\n","    #Validacion cruzada\n","    for train_index, test_index in tscv.split(X_train):\n","      X_train_, X_test_ = X_train[train_index], X_train[test_index]\n","      y_train_, y_test_ = Y_train_s[train_index], Y_train_s[test_index]\n","\n","      # Crear el modelo de LSTM\n","      model = create_model(int(neurons), float(dropout),int(layers),float(learning_rate))\n","\n","      #Entrenamiento\n","      start = time.time()\n","      hist=model.fit(X_train_, y_train_, batch_size=int(batch_size), epochs=int(epochs), verbose=0, validation_data=(X_test_, y_test_),callbacks=[early_stop],use_multiprocessing=True)\n","      end = time.time()\n","\n","      #Evaluacion del modelo\n","      score = model.evaluate(X_test_, y_test_, verbose = 0)\n","      scores.append(score)\n","      #Error en conjunto de test\n","      score_test = model.evaluate(X_test, Y_test, verbose = 0)\n","      scores_test.append(score_test)\n","\n","      #guardar modelo keras\n","      models.append(model)\n","\n","      #Tiempo de la validadion cruzada\n","      time_val= end- start\n","      times.append(time_val)\n","\n","    return {'loss': np.mean(scores),\n","            'status': STATUS_OK,\n","            'model': model,\n","            'params': params,\n","            'time':times,\n","            'scores_test': scores_test,\n","            'hist':hist,\n","            'scores': scores,\n","            'models':models}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WhU97rWWNT_"},"outputs":[],"source":["#Se define el espacio de busqueda de hiperparametros para el modelo\n","space = {\n","    'epochs': 800,\n","    'batch_size':  hp.quniform('batch_size', 10, 100, 10),\n","    'optimizer':'adam',\n","    'neurons': hp.quniform('neurons', 64, 576, 32),\n","    'dropout':hp.uniform('dropout', 0.4, 0.7),\n","    'layers': hp.quniform('layers', 1, 3, 1),\n","    'learning_rate': hp.uniform('learning_rate', 0.00001, 0.00005),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZ0bcqJdkGN5"},"outputs":[],"source":["'''\n","#Se define el espacio de busqueda de hiperparametros para el modelo\n","space = {\n","    'epochs': 800,\n","    'batch_size': 90,\n","    'optimizer':'adam',\n","    'neurons': 64,\n","    'dropout': 0.6965950072420362,\n","    'layers': 3,\n","    'learning_rate': 2.1104024068839047e-05,\n","}\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0TgeLtYgJIY"},"outputs":[],"source":["#Optimización bayesiana\n","trials = Trials()\n","best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n","\n","print(\"Los mejores hiperparámetros son: \", best)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-LkIIjzQ7ro"},"outputs":[],"source":["#Obtener el valor de la función objetivo del mejor ensayo\n","best_trial = trials.best_trial\n","hist = best_trial['result']['hist']\n","best_model = best_trial['result']['model']\n","scores_model = best_trial['result']['scores']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wdfti6COV2GL"},"outputs":[],"source":["# Obtener una lista de los resultados de todas las evaluaciones\n","all_results = [trial['result'] for trial in trials]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzzcaE4MqjZg"},"outputs":[],"source":["# Mejores hiperparámetros encontrados\n","best_params = space_eval(space, best)\n","best_params"]},{"cell_type":"markdown","metadata":{"id":"PoXytm9xepyx"},"source":["# 6.Evaluacion del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WU20oJJt7bBv"},"outputs":[],"source":["plt.figure(figsize=(8,4))\n","plt.plot(hist.history['loss'], color = 'orange')\n","plt.plot(hist.history['val_loss'])\n","plt.title('Optimized Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'test'], loc='best')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-qQvke-of30"},"outputs":[],"source":["#Se realiza prediccion\n","results = best_model.predict(X_test)\n","\n","results = results.reshape(shape_y_test)\n","y_grafico = Y_test.reshape(shape_y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJU6XZvNof32"},"outputs":[],"source":["#Grafico de prediccion con el valor real\n","tiempo=[x for x in range(y_grafico.shape[0])]\n","plt.figure(figsize=(150,4))\n","plt.plot(tiempo,results)\n","plt.ylabel('Global_active_power', size=15)\n","plt.plot(tiempo,y_grafico)\n","plt.xlabel('Time step', size=15)\n","plt.legend(['Prediccion','Real'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yH9bmzulWWER"},"outputs":[],"source":["#Metricas de precision del modelo keras\n","score = best_model.evaluate(X_test, Y_test, verbose = 0)\n","print('MSE:', score[1])\n","print('RMSE:', score[2])\n","print('MAE:', score[3])\n","print('MAPE:', score[4])"]},{"cell_type":"markdown","metadata":{"id":"UcxinE4q4n3B"},"source":["#Exportar resultados"]},{"cell_type":"markdown","metadata":{"id":"HSSOElBiSJ6j"},"source":["Se guardan los resultados la optimizacion bayesiana"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"el6nmqt8yYcE"},"outputs":[],"source":["#Del objeto all_results donde estan los resultados de cada trial de la optimizacion bayesiana se obtiene los parametros para exportalos a un csv\n","results=[]\n","trial=0 #Nº de intento de optimizacion bayesiana\n","\n","for result in all_results:\n","  k=0 #validacion cruzada\n","  trial+=1\n","  i=0\n","  for time in result['time']:\n","    k+=1\n","    nameModel = \"LSTM_Wallmart\" + \"_\"+str(trial)+\"_\"+str(k)  + \"_\"+str(result['params']['epochs'])+\"_\"+str(result['params']['batch_size'])+\"_\"+str(result['params']['layers'])+\"_\"+str(result['params']['neurons'])+\"_\"+str(round(result['params']['dropout'],2))+\"_\"+str(result['params']['optimizer'])+\"_\"+str(result['params']['learning_rate'])\n","    results.append([nameModel,trial,k,time,\n","                    result['scores_test'][i][0],result['scores_test'][i][2],result['scores_test'][i][3],result['scores_test'][i][4]]\n","                   )\n","    i+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"42RrcsC43uO2"},"outputs":[],"source":["#Se crea dataframe\n","results_csv=pd.DataFrame(results,columns=['nameModel','trial_optimizacion_bayesiana','Step_validacion','time','MSE','RMSE','MAE','MAPE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FmG0_peW2pHP"},"outputs":[],"source":["results_o = results_csv.sort_values(by='MSE', ascending=True)\n","results_o.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtyQkZX812j5"},"outputs":[],"source":["# Exportar el DataFrame como CSV\n","results_csv.to_csv('results_LSTM_Wallmart.csv')"]},{"cell_type":"markdown","metadata":{"id":"9bNdR66vSPLU"},"source":["Se guarda cada modelo keras en un objeto Json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbGwY-Zz1-47"},"outputs":[],"source":["import os\n","# Ruta de la carpeta que deseas crear\n","carpeta_destino = '/content/gdrive/MyDrive/Tesis/Datos/result_LSTM'\n","\n","# Verificar si la carpeta ya existe\n","if not os.path.exists(carpeta_destino):\n","    # Crear la carpeta si no existe\n","    os.makedirs(carpeta_destino)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFEhYABX14-m"},"outputs":[],"source":["#Del objeto all_results donde estan los resultados de cada trial de la optimizacion bayesiana se obtiene los modelos para exportalos a un Json\n","results=[]\n","trial=0 #Nº de intento de optimizacion bayesiana\n","\n","for result in all_results:\n","  k=0 #validacion cruzada\n","  trial+=1\n","  i=0\n","  for model in result['models']:\n","    k+=1\n","    nameModel = \"2LSTM_Wallmart\"+\"_\"+str(trial)+\"_\"+str(k) + \"_\"+str(result['params']['epochs'])+\"_\"+str(result['params']['batch_size'])+\"_\"+str(result['params']['layers'])+\"_\"+str(result['params']['neurons'])+\"_\"+str(round(result['params']['dropout'],2))+\"_\"+str(result['params']['optimizer'])+\"_\"+str(result['params']['learning_rate'])\n","    model_json = model.to_json()\n","    ruta = os.path.join(carpeta_destino, nameModel + \".json\")\n","    with open(ruta, \"w\") as json_file:\n","        json_file.write(model_json)\n","    # serialize weights to HDF5\n","    save_w=nameModel+\".\"+\"h5\"\n","    weights_path = os.path.join(carpeta_destino, save_w)\n","    model.save_weights(weights_path)\n","    i+=1"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["6RP-4WruJKgE","NnR5PQgjnb0w","jm_8gyGw2l8v","NDnWUzQzmqBj","7cWkYwLPYSLg","PoXytm9xepyx"],"machine_shape":"hm","provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMhR8mgjtZGfsToNmqIbNFM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}